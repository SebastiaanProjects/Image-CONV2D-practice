{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear all,\n",
    "\n",
    "\n",
    "I designed a simple exercise in order to speed up the basic learning you need to do before starting your master thesis. \n",
    "\n",
    "\n",
    "I want you to make a two layer network. You can use torch.nn.Linear, Conv1D or Conv2D. Your input and labels are randomly generated. So your input can be an image like WxHxC (width x height x channel), e.g. 4x5x3 or signal like WxC (width x channel) e.g. 5x4 and output is labels such as [0, 1, 2]. \n",
    "\n",
    "- Write the whole training and evaluation pipeline: You should calculate loss, backpropagate to calculate gradients and update your weights\n",
    "\n",
    "- Save training and validation loss in tensorboard and monitor the graphs\n",
    "\n",
    "- Save training and validation accuracy in tensorboard and monitor the graphs\n",
    "\n",
    "- Visualize your activations\n",
    "\n",
    "- Run it on your computer \n",
    "\n",
    "- Run it on Colab\n",
    "\n",
    "- Run it on Snellius\n",
    "\n",
    "- Push your code in your Github page together with Colab notebook and give the link to me before our meeting\n",
    "\n",
    "- Bonus: Check if you can connect to Snellius or Google Colab via vscode. vscode has a very good debugger and editor. In this case, you can speed up your development. Here is some info for colab, but I have not tested myself. You don't have to use vscode. Whatever IDE you prefer is fine.\n",
    "\n",
    "\n",
    "We can have a meeting in two weeks and see what kind of trouble or issues you encountered. Please reach out to each other for help. You can learn from my colab notebooks, pytorch tutorials such as this one, or chatGPT. \n",
    "\n",
    "Here is how to access Snellius GPU and how to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import save, load\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam, SGD\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing training data, using an already existing dataset named MNIST    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = datasets.MNIST(root=\"data\", download=True, train=True, transform=ToTensor())\n",
    "#dataset = DataLoader(train, 32) #batches of 32 images\n",
    "\n",
    "train = datasets.MNIST(root=\"data\", download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split the dataset into training, test, and validation sets\n",
    "total_size = len(train)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "test_size = (total_size - train_size) // 2  # 10% for testing\n",
    "val_size = total_size - train_size - test_size  # 10% for validation\n",
    "\n",
    "train_set, test_set, val_set = random_split(train, [train_size, test_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for each set\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "#shape of image is given as [1, 28, 28]\n",
    "#classes are 0-9\n",
    "\n",
    "commit_test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create image classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #ensures initialization logic of the nn.Module\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (3,3)), #input channel = 1, since black and white. 32 filters. shape 3 by 3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (3,3)), #input channel is now 32, output = 64, still same shape\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,(3,3)), #same\n",
    "            nn.Flatten(), #always applied after last convolutional layer, converts output of convolutional layers into 1d tensor which can be used as input for a linear layer\n",
    "            nn.Linear(64*(28-6)*(28-6), 10)# for each Conv2d we are deleting the outer two pixels layers of the data, since that how the pooling works. So we need to make up for that here by substracting 2 from each layer (2*3 = 6) from the original size (28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the neural network, loss and optimizer. Also create platform to keep track of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ImageClassifier().to('cpu') #use cuda if you have a separate GPU \n",
    "#opt = Adam(clf.parameters(), lr=1e-3) #Adam is one of the optimzers. Can use others. learningrate is now 0.001\n",
    "opt = SGD(clf.parameters(), lr= 1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss() #possible loss function\n",
    "#loss_fn = nn.MSELoss()\n",
    "#RMSE_loss = torch.sqrt(loss_fn(prediction, target))\n",
    "#RMSE_loss.backward()\n",
    "writer = SummaryWriter('logs')  # 'logs' is the directory where TensorBoard logs will be stored\n",
    "writer1 = SummaryWriter(f'runs/MNIST/tryingout_tensorboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss is 0.22443144023418427\n",
      "Epoch: 1 loss is 0.3028142750263214\n",
      "Epoch: 2 loss is 0.05996200442314148\n",
      "Epoch: 3 loss is 0.2948084771633148\n",
      "Epoch: 4 loss is 0.3032251000404358\n",
      "Epoch: 5 loss is 0.1654440462589264\n",
      "Epoch: 6 loss is 0.15880417823791504\n",
      "Epoch: 7 loss is 0.16517670452594757\n",
      "Epoch: 8 loss is 0.391103059053421\n",
      "Epoch: 9 loss is 0.15803726017475128\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": #run deep learning for epochs\n",
    "    loss_tracker = []\n",
    "    for epoch in range(10): #train for 10 epochs\n",
    "        for batch in train_loader:\n",
    "            X, y = batch\n",
    "            X, y = X.to('cpu'), y.to('cpu')\n",
    "            yhat = clf(X) #this will be our predictions\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            #apply backpropagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward() #calculate the gradients\n",
    "            opt.step()\n",
    "            writer.add_scalar('Loss/Train', loss, global_step=epoch)\n",
    "\n",
    "        #print the loss for every batch\n",
    "        loss_tracker.append(loss.item())\n",
    "        plt.plot(loss_tracker)     \n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        print(f\"Epoch: {epoch} loss is {loss.item()}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09312333911657333\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    X, y = batch\n",
    "    X, y = X.to('cpu'), y.to('cpu')\n",
    "    yhat = clf(X) #this will be our predictions\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    #apply backpropagation\n",
    "    #opt.zero_grad()\n",
    "    #loss.backward() #calculate the gradients\n",
    "    #opt.step()\n",
    "    writer.add_scalar('Loss/test', loss, global_step=epoch)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (1983852640.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorboard --logdir=logs\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open('model_state.pt', 'wb') as f: #wb = write binairy\n",
    "    save(clf.state_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oefenTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
